---
title: "Linear models"
---

```{r eval = TRUE,  message=F, include=F, warning=F, purl=F, results="hide"}
knitr::purl('linear.Rmd', documentation = F)
```

```{r echo=FALSE}
xaringanExtra::use_clipboard()
```

```{r echo=FALSE, purl=F}
xfun::embed_file('linear.Rmd')
```

```{r echo=FALSE, purl=F}
xfun::embed_file('linear.R')
```

```{r echo=FALSE, purl=F}
xfun::embed_file('Data/shagLPI.csv')
```

```{r echo=FALSE, purl=F}
xfun::embed_file('Data/Weevil_damage.csv')
```

```{r,  eval=T, warning=F, message=F}
library(Hmisc)
library(corrplot)
library(MASS)
library(car)
library(interactions)
library(yarrr)
library(tidyr)
library(readr)
library(lme4)
library (lmerTest)
library(nlme)
library(gvlma)
```

In the coming two weeks, we will take some time on basic concepts behind some simple linear models.

# Linear regression

## Definition

Let's start with a a simple correlation among two variables:

```{r,  eval=T, warning=F, message=F}
# correlation
rairuoho<-read.table('https://www.dipintothereef.com/uploads/3/7/3/5/37359245/rairuoho.txt',header=T, sep="\t", dec=".")
cor.test(rairuoho$day6, rairuoho$day7)
```

We can repeat it for all days by building a Pearson coefficient matrix:

```{r,  eval=T, warning=F, message=F}
corr<-cor(rairuoho[,1:6])
corr # cor.test does not work on Matrix
```          

Which however doesn't give us the p-values of those correlations. Many packages offer  a better visualization of this matrix, such as `corrplot`: 

```{r,  eval=T, warning=F, message=F}
p.val<-rcorr(as.matrix(rairuoho[,1:6]))$P
corrplot(corr,type='upper',method='color', addCoef.col = "black", p.mat=as.matrix(p.val), sig.level = 0.05,title = "Correlation Matrix", mar = c(2,0,2,0), diag=F)
```

Previously, we saw that this type of 'relationship' (a linear model `lm`) between two variables can be added to a scatterplot using the function `abline` 

```{r,  eval=T, warning=F, message=F}
plot(rairuoho$day6, rairuoho$day7)
abline(lm(rairuoho$day7~rairuoho$day6), col="red", lwd=2)
# remember `ggplot`
# ggplot(rairuoho, aes(x = day6, y = day7)) + 
#  geom_point() +
#  stat_smooth(method = "lm", col = "red")
```

Indeed our correlation derived from a simple __linear regression__. In **Model I regression** (indeed we have Model II when two variable in the regression equation are random , i.e. no controlled by the researcher), it is used to predict a quantitative outcome of a dependent variable $y$ on the basis of one single independent predictor variable $x$. The goal is to build a mathematical model (or formula) that defines $y$ as a function of the $x$ variable.

Once, we built a statistically significant model, it’s possible to use it for predicting future outcome on the basis of new $x$ values.

## Formula and basics

The formula of linear regression can be written as follows: $$ y = \beta_0 + \beta_1*x + \epsilon $$

where:

+ $\beta_0$ and $\beta_1$ are known as the regression __beta coefficients__ or __parameters__:

  + $\beta_0$ is the __intercept__ of the regression line; that is the predicted value when _x = 0_.
  + $\beta1$ is the __slope__ of the regression line.
  
+ $\epsilon$ is the __error term__ (also known as the __residual errors__)

The figure below illustrates the linear regression model, where:

+ the best-fit regression line is in blue
+ the intercept ($\beta_0$) and the slope ($\beta_1$) are shown in green
+ the error terms ($\epsilon$) are represented by vertical red lines

<center>
![](Figures/linear-regression.png){width=50%}
</center>


From the figure above, it can be seen that not all the data points fall exactly on the fitted regression line. Some of the points are above the blue curve and some are below it; overall, the residual errors ($\epsilon$) have approximately mean zero.

**TERMINOLOGY ALERT**

The sum of the squares of the residual errors are called the __Residual Sum of Squares__ or __RSS__.

The average variation of points around the fitted regression line is called the __Residual Standard Error (RSE)__. This is one the metrics used to evaluate the overall quality of the fitted regression model. The lower the RSE, the better it is.

Since the mean error term is zero, the outcome variable y can be approximately estimated as follow:

$$y= \beta_0+\beta_1*x$$

Mathematically, the beta coefficients ($\beta_0$ and $\beta_1$) are determined so that the **RSS** is as minimal as possible. This method of determining the beta coefficients is technically called __least squares__ regression or __ordinary least squares__ (OLS) regression.

Once, the beta coefficients are calculated, a *t*-test is performed to check whether or not these coefficients are significantly different from zero. A non-zero beta coefficients means that there is a significant relationship between the predictors ($x$) and the outcome variable ($y$).

## Computation

Finally! The simple linear regression tries to find the best line to predict $y$ on the basis of $x$.

In the `iris` data set, a linear model equation can be written as follow: 

$Petal.Width = \beta_0 + \beta_1 * Petal.Length$

In R language, it is translated using the  function `lm` and in order to determine the beta coefficients of the linear model:

```{r,  eval=T, warning=F, message=F}
model1 <- lm(Petal.Width ~ Petal.Length, data = iris)
model1$coefficients
```
The results show the intercept ($\beta_0$) and the slope ($\beta_1$), i.e. the  beta coefficients for the `Petal.Length` variable

```{r,  eval=T, warning=F, message=F}
ggplot(iris, aes(x = Petal.Length, y = Petal.Width)) +
  geom_point(aes(fill=Species),shape = 21, size=5) +
  stat_smooth(method = "lm", col = "blue")
```

## Interpretation

From the output above:

+ the estimated regression line equation can be written as follow: $Petal.Width = -0.3631 + 0.4158*Petal.Length$

+ the intercept ($\beta_0$) is $-0.3631$. It can be interpreted as the predicted width of petal for a length of petal equal to zero. Regression through the origin is when you force the intercept of a regression model to equal zero. It’s also known as fitting a model without an intercept (e.g., the intercept-free linear model $y = \beta_1*x$ is equivalent to the model $y = \beta_0 + \beta_1*x$ with $\beta_0=0$). Knowing that the **true relationship** between your predictors and the expected value of your dependent variable has to pass through the origin would be a good reason for forcing the estimated relationship through the origin if you knew for certain what the true relationship was (be careful very rare cases where it is justified to remove the intercept).


+ the regression beta coefficient for the variable `Petal.Length` ($\beta1$), also known as the slope, is $0.4158$. This means that, for one unit of $Petal.Length$, we can expect an increase of $0.4158$ units in $Petal.Width$. 

## Model assessment

Before using our model to predict $Petal.Width$, we should make sure that this model is statistically significant, that is:

+ there is a statistically significant relationship between the predictor and the outcome variables

+ the model that we built fits very well the data in our hand.

## Model summary

Using our `lm` model, we get a bit more than just the coefficient ($\beta0$) and ($\beta1$) 


```{r,  eval=T, warning=F, message=F}
summary(model1)
```

Looking at only the p-values, this simple model seems to fit the data very well. But the output tells us much more. The summary outputs shows 6 components, including:

+ **Call** shows the function call used to compute the regression model.
+ **Residuals** provide a quick view of the distribution of the residuals, which by definition have a mean zero. Therefore, the median should not be far from zero, and the minimum and maximum should be roughly equal in absolute value.
+ **Coefficients** shows the regression beta coefficients and their statistical significance. Predictor variables, that are significantly associated to the outcome variable, are marked by stars.
+ **Residual standard error** (RSE), **R-squared** (R2) and the **F-statistic** are metrics that are used to check how well the model fits to our data.

### Coefficients significance

The coefficients table, in the model statistical summary, shows:

+ the estimates of the **beta coefficients**
+ the **standard errors** (SE), which defines the accuracy of beta coefficients. For a given beta coefficient, the SE reflects how the coefficient varies under repeated sampling. It can be used to compute the confidence intervals and the t-statistic.
+ the **_t_-statistic** and the associated **_p_-value**, which defines the statistical significance of the beta coefficients.


**t-statistic and p-values**

For a given predictor, the t-statistic (and its associated *p*-value) tests whether or not there is a statistically significant relationship between a given predictor and the outcome variable, that is whether or not the beta coefficient of the predictor is significantly different from zero.

The statistical hypotheses are as follow:

+ Null hypothesis (H~0~): the coefficients are equal to zero (i.e., no relationship between x and y)
+ Alternative Hypothesis (H~1~): the coefficients are not equal to zero (i.e., there is some relationship between $x$ and $y$)

Mathematically, for a given beta coefficient ($\beta$), the _t_-test is computed as $t = (\beta - 0)/SE(\beta)$, where $SE(\beta)$ is the SE of the coefficient $\beta$. Simply said, the t-statistic measures the number of standard deviations that $\beta$ is away from 0. Thus a large _t_-statistic will produce a small p-value (=different).

The higher the _t_-statistic (and the lower the p-value), the more significant the predictor. The symbols to the right (***) visually specifies the level of significance. The line below the table shows the definition of these symbols; one star means 0.01 < p < 0.05. The more the stars beside the variable’s p-value, the more significant the variable.

A statistically significant coefficient indicates that there is an association between the predictor ($x$) and the outcome ($y$) variable.

The _t_-statistic is a very useful guide for whether or not to include a predictor in a model. High t-statistics (which go with low p-values near 0) indicate that a predictor should be retained in a model, while very low _t_-statistics indicate a predictor could be dropped (Bruce and Bruce 2017).

**Standard errors and confidence intervals**

The standard error measures the variability/accuracy of the beta coefficients. It can be used to compute the confidence intervals of the coefficients.

For example, the 95% confidence interval for the coefficient $\beta1$ is defined as $\beta1 +/- 2*SE(\beta1)$, where:

the lower limits of $\beta_1 = \beta_1 - 2*SE(\beta_1) = 0.415 - 2*0.009 = 0.397$

the upper limits of $\beta1 = \beta1 + 2*SE(\beta1) = 0.415 + 2*0.009 = 0.435$

That is, there is approximately a 95% chance that the interval [0.397, 0.435] will contain the true value of $\beta1$. Similarly the 95% confidence interval for $\beta0$ can be computed as $\beta0 +/- 2*SE(\beta0)$.

To get this information, either you calculate by hands or you simply call:

```{r,  eval=T, warning=F, message=F}
confint(model1)
```

### Model accuracy


Once you identified that, at least, one predictor variable is significantly associated the outcome, you should continue the diagnostic by checking how well the model fits the data. This process is also referred to as the *goodness-of-fit*

The overall quality of the linear regression fit can be assessed using the following three parameters, displayed in the model summary:

1. **The Residual Standard Error (RSE)**

The **RSE** (also known as the model sigma) is the **residual variation**, representing the average variation of the observations points around the fitted regression line. This is the **standard deviation of residual errors**.

RSE provides an absolute measure of patterns in the **data that can’t be explained by the model**. When comparing two models, the model with the small RSE is a good indication that this model fits the best the data.

Dividing the RSE by the average value of the outcome variable will give you the prediction error rate, which should be as small as possible.

In our example, RSE = `0.2065`, meaning that the observed Petal.width values deviate from the true regression line by approximately `0.2065` units in average.

Whether or not an RSE of `0.2065` units is an acceptable prediction error is subjective and depends on the problem context. However, we can calculate the percentage error. In our data set, the mean value of Petal.Width is 1.1993, and so the percentage error is 0.2065/1.1993 = 17%.

```{r,  eval=T, warning=F, message=F}
sigma(model1)*100/mean(iris$Petal.Width)
```

2. **The R-squared ($R^2$)**

The **R-squared** $R^2$ ranges from 0 to 1 and represents the **proportion of information (i.e. variation) in the data that can be explained by the model**. The **adjusted $R^2$ adjusts $R^2$ with the degrees of freedom**.

The $R^2$ measures, how well the model fits the data. For a simple linear regression, $R^2$ is the square of the **Pearson correlation coefficient**.

A high value of $R^2$ is a good indication. However, as the value of $R^2$ tends to increase when more predictors are added in the model, such as in **multiple linear regression mode**l, you should mainly consider the adjusted $R^2$**, which is a penalized $R^2$ for a higher number of predictors.

+ An (adjusted) $R^2$ that is close to 1 indicates that a large proportion of the variability in the outcome has been explained by the regression model.

+ A number near 0 indicates that the regression model did not explain much of the variability in the outcome.

3. **F-statistic**

The **F-statistic** gives the overall significance of the model. It assess whether **at least one predictor variable has a non-zero coefficient**.

In a simple linear regression, this test is not really interesting since it just duplicates the information in given by the *t*-test, available in the coefficient table. In fact, the *F*-test is identical to the square of the t-test: $1882 = (43.387)^2$. That would be true in any model with 1 degree of freedom.

The F-statistic becomes **more important** once we start **using multiple predictors** as in multiple linear regression.

A large F-statistic will corresponds to a statistically significant *p*-value (p < 0.05). In our example, the F-statistic equal 1882 producing a p-value of < 2.2e-16, which is highly significant.

# Multiple regression

**Multiple linear regression** is just an **extension of simple linear regression** used to predict an outcome variable ($y$) on the basis of multiple distinct predictor variables ($x$).

With three predictor variables ($x$), the prediction of $y$ is expressed by the following equation:

$$y = \beta_0 + \beta_1*x_1 + \beta_2*x_2 + \beta_3*x_3$$

The “$\beta$” values are called the regression weights (or beta coefficients). They measure the association between the predictor variable and the outcome. “$\beta_j$” can be interpreted as the average effect on $y$ of a one unit increase in “$x_j$”, holding all other predictors fixed.

## Fitting the model

R provides comprehensive support for multiple linear regression. We want to build a model for estimating `Petal.Width` based on data we get on `Petal.Length`, `Sepal.Length`, and `Sepal.Width`. 


$$Petal.Width = \beta_0 + \beta_1*Petal.Length + \beta_2*Sepal.Length + \beta_3*Sepal.Width$$
You can now easily compute this in R as follow:


```{r,  eval=T, warning=F, message=F}
fit1 <- lm(Petal.Width ~ Petal.Length + Sepal.Width + Sepal.Length, data = iris)
summary(fit1)
```

```{r,  eval=F, warning=F, message=F}
# Other useful functions
coefficients(fit1) # model coefficients
confint(fit1, level=0.95) # CIs for model parameters
fitted(fit1) # predicted values
residuals(fit1) # residuals
anova(fit1) # anova table
vcov(fit1) # covariance matrix for model parameters
influence(fit1) # regression diagnostics
```

## Regression diagnostics

See [here](https://www.statmethods.net/stats/rdiagnostics.html) for functions and packages

### Diagnostic plots

Diagnostic plots provide checks for heteroscedasticity, normality, and influential observations.

```{r,  eval=T, warning=F, message=F}
# diagnostic plots
layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs/page
plot(fit1)
```

### Outliers

```{r,  eval=T, warning=F, message=F}
# Assessing Outliers
outlierTest(fit1) # Bonferonni p-value for most extreme obs
qqPlot(fit1, main="QQ Plot") #qq plot for studentized resid
leveragePlots(fit1) # leverage plots
```

### Influential observations

```{r,  eval=T, warning=F, message=F}
# Influential Observations
# added variable plots
avPlots(fit1)
# Cook's D plot
# identify D values > 4/(n-k-1)
cutoff <- 4/((nrow(iris)-length(fit1$coefficients)-2))
plot(fit1, which=4, cook.levels=cutoff)
# Influence Plot
influencePlot(fit1, id.method="identify", main="Influence Plot", sub="Circle size is proportial to Cook's Distance" )
```

### Non-normality

```{r,  eval=T, warning=F, message=F}
# Normality of Residuals
# qq plot for studentized resid
qqPlot(fit1, main="QQ Plot")
# distribution of studentized residuals
sresid <- studres(fit1)
hist(sresid, freq=FALSE,
   main="Distribution of Studentized Residuals")
xfit<-seq(min(sresid),max(sresid),length=40)
yfit<-dnorm(xfit)
lines(xfit, yfit)
```

### Non-constant Error Variance

```{r,  eval=T, warning=F, message=F}
# Evaluate homoscedasticity
# non-constant error variance test
ncvTest(fit1)
# plot studentized residuals vs. fitted values
spreadLevelPlot(fit1)
```

### Multi-collinearity


```{r,  eval=T, warning=F, message=F}
# Evaluate Collinearity
vif(fit1) # variance inflation factors
sqrt(vif(fit1)) > 2 # problem?
```

### Nonlinearity

```{r,  eval=T, warning=F, message=F}
# Evaluate Nonlinearity
# component + residual plot
crPlots(fit1)
# Ceres plots
ceresPlots(fit1)
```

### Non-independence of Errors

```{r,  eval=T, warning=F, message=F}
# Test for Autocorrelated Errors
durbinWatsonTest(fit1)
```

### Additional Diagnostic Help

The `gvlma( )` function in the `gvlma` package, performs a global validation of linear model assumptions as well separate evaluations of skewness, kurtosis, and heteroscedasticity.

```{r,  eval=T, warning=F, message=F}
gvmodel <- gvlma(fit1)
summary(gvmodel)
```

This tests for the linear model assumptions and helpfully provides information on other assumptions. In this case we are going to look at the heteroskedasticity decisions, which has been identified as not being satisfied. We therefore reject the null hypothesis and state that there is heteroskedasticity in this model at the 5% significance level.
